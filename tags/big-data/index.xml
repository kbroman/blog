<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Big Data on the stupidest thing...</title>
    <link>http://kbroman.org/blog/tags/big-data/</link>
    <description>Recent content in Big Data on the stupidest thing...</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>The text in this work is licensed under CC-BY-4.0, https://creativecommons.org/licenses/by/4.0/legalcode; code licensed under the MIT License</copyright>
    <lastBuildDate>Thu, 11 May 2017 23:50:00 -0500</lastBuildDate>
    <atom:link href="http://kbroman.org/blog/tags/big-data/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>reading/writing biggish data, revisited</title>
      <link>http://kbroman.org/blog/2017/05/11/reading/writing-biggish-data-revisited/</link>
      <pubDate>Thu, 11 May 2017 23:50:00 -0500</pubDate>
      
      <guid>http://kbroman.org/blog/2017/05/11/reading/writing-biggish-data-revisited/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://twitter.com/mattdowle?lang=en&#34;&gt;Matt Dowle&lt;/a&gt; encouraged me to follow up on my &lt;a href=&#34;../../2017/04/30/sqlite-feather-and-fst/&#34;&gt;post about sqlite, feather, and fst&lt;/a&gt;. One thing to emphasize is that &lt;code&gt;saveRDS&lt;/code&gt;, by default, uses compression. If you use &lt;code&gt;compress=FALSE&lt;/code&gt; you can skip that and it goes &lt;em&gt;much&lt;/em&gt; faster. See, for example, &lt;a href=&#34;https://blog.h2o.ai/2016/04/fast-csv-writing-for-r/&#34;&gt;his post on “Fast csv writing for R”&lt;/a&gt;. Also see his &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki/talks/BARUG_201704_ParallelFread.pdf&#34;&gt;slides from a recent presentation on parallel fread&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ll first generate the same data that I was using before. And note, as &lt;a href=&#34;https://twitter.com/shabbychef&#34;&gt;@shabbychef&lt;/a&gt; &lt;a href=&#34;https://twitter.com/shabbychef/status/858892435820130304&#34;&gt;mentioned on twitter&lt;/a&gt;, my iid simulations mean that compression isn’t likely to be useful, &lt;a href=&#34;../../2017/04/30/sqlite-feather-and-fst/&#34;&gt;as we saw in my previous post&lt;/a&gt;. So don’t assume that these results apply generally; compression is useful much of the time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ind &amp;lt;- 500
n_snps &amp;lt;- 1e5
ind_names &amp;lt;- paste0(&amp;quot;ind&amp;quot;, 1:n_ind)
snp_names &amp;lt;- paste0(&amp;quot;snp&amp;quot;, 1:n_snps)
sigX &amp;lt;- matrix(rnorm(n_ind*n_snps), nrow=n_ind)
sigY &amp;lt;- matrix(rnorm(n_ind*n_snps), nrow=n_ind)
dimnames(sigX) &amp;lt;- list(ind_names, paste0(snp_names, &amp;quot;.X&amp;quot;))
dimnames(sigY) &amp;lt;- list(ind_names, paste0(snp_names, &amp;quot;.Y&amp;quot;))
db &amp;lt;- cbind(data.frame(id=ind_names, stringsAsFactors=FALSE),
            sigX, sigY)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s look at the time to write an RDS file, when compressed and when not. I’m again going to cache my results and just tell you what happened.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rds_file &amp;lt;- &amp;quot;db.rds&amp;quot;
saveRDS(db, rds_file, compress=FALSE)
rds_comp_file &amp;lt;- &amp;quot;db_comp.rds&amp;quot;
saveRDS(db, rds_comp_file)
db_copy1 &amp;lt;- readRDS(rds_file)
db_copy2 &amp;lt;- readRDS(rds_comp_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Writing the data to an RDS file took 5.5 sec when uncompressed and 51.4 sec when compressed. Reading them back in took 2.4 sec for the uncompressed file and 11.0 sec for the compressed file. The uncompressed RDS file was 805 MB, while the compressed one was 769 MB.&lt;/p&gt;
&lt;p&gt;So, &lt;em&gt;holy crap&lt;/em&gt; reading and writing the RDS files is fast when you use &lt;code&gt;compress=FALSE&lt;/code&gt;. Don’t tell your system administrator I said this, but if you’re working on a server with loads of disk space, for sure go with &lt;code&gt;compress=FALSE&lt;/code&gt; with your RDS files. On your laptop where uncompressed RDS files might get in the way of your music and movie libraries, you might want to use the compression.&lt;/p&gt;
&lt;div id=&#34;how-about-csv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How about CSV?&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dirk.eddelbuettel.com/&#34;&gt;Dirk Eddelbuettel&lt;/a&gt; suggested that I might just use a plain CSV file, since &lt;code&gt;data.table::fread&lt;/code&gt; and &lt;code&gt;data.table::fwrite&lt;/code&gt; are so fast. How fast?&lt;/p&gt;
&lt;p&gt;To make use of the multi-threaded version of &lt;a href=&#34;https://github.com/Rdatatable/data.table/wiki&#34;&gt;data.table&lt;/a&gt;’s &lt;code&gt;fread&lt;/code&gt;, I need version 1.10.5 which is &lt;a href=&#34;https://github.com/rdatatable/data.table&#34;&gt;on GitHub&lt;/a&gt;. The version on &lt;a href=&#34;https://cran.r-project.org&#34;&gt;CRAN&lt;/a&gt; (&lt;a href=&#34;https://cran.r-project.org/package=data.table&#34;&gt;1.10.4&lt;/a&gt;) has multi-threaded &lt;code&gt;fwrite&lt;/code&gt; but only single-threaded &lt;code&gt;fread&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But the GitHub version needs to be compiled with OpenMP, and after a lot of screwing around to do that, I ended up getting segfaults from &lt;code&gt;fwrite&lt;/code&gt;, so I just dumped this plan.&lt;/p&gt;
&lt;p&gt;So we’ll look at multi-threaded &lt;code&gt;fwrite&lt;/code&gt; but only single-threaded &lt;code&gt;fread&lt;/code&gt;. But we can all look forward to the multi-threaded &lt;code&gt;fread&lt;/code&gt; in the near future.&lt;/p&gt;
&lt;p&gt;For &lt;code&gt;fwrite&lt;/code&gt;, the number of threads is controlled by the argument &lt;code&gt;nThread&lt;/code&gt;. The default is to call &lt;code&gt;data.table::getDTthreads()&lt;/code&gt; which detects the maximum number of cores. On my Mac desktop at work, that’s 24. I’m going to hard-code it in.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;csv_file &amp;lt;- &amp;quot;db.csv&amp;quot;
library(data.table)
fwrite(db, csv_file, quote=FALSE)
db_copy3 &amp;lt;- data.table::fread(csv_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That took 41.6 sec to write and 55.0 sec to read, and the file size is 1818 MB.&lt;/p&gt;
&lt;p&gt;How about if I set &lt;code&gt;nThread=1&lt;/code&gt; with &lt;code&gt;fwrite&lt;/code&gt;?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fwrite(db, csv_file, quote=FALSE, nThread=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Single-threaded, &lt;code&gt;fwrite&lt;/code&gt; took 69.1 sec.&lt;/p&gt;
&lt;p&gt;But the data set is 500 rows by 200k columns. How about if I used the transpose?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t_db &amp;lt;- cbind(data.frame(snp=rep(snp_names, 2),
                         signal=rep(c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;), each=n_snps),
                         stringsAsFactors=FALSE),
              rbind(t(sigX), t(sigY)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to write and read this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;csv_t_file &amp;lt;- &amp;quot;db_t.csv&amp;quot;
fwrite(t_db, csv_t_file, quote=FALSE, nThread=24)
t_db_copy &amp;lt;- fread(csv_t_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That took 8.3 sec to write and 26.6 sec to read, and the file size is 1818 MB.&lt;/p&gt;
&lt;p&gt;And how about if I do &lt;code&gt;fwrite&lt;/code&gt; single-threaded?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fwrite(t_db, csv_t_file, quote=FALSE, nThread=1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Single-threaded, the transposed data took 30.2 sec to write.&lt;/p&gt;
&lt;p&gt;(I’m not even going to try &lt;code&gt;read.csv&lt;/code&gt; and &lt;code&gt;write.csv&lt;/code&gt;. I’ll leave that to the reader.)&lt;/p&gt;
&lt;p&gt;Here’s a summary of the times:&lt;/p&gt;
&lt;style type=&#34;text/css&#34;&gt;.table { width: 100%; }&lt;/style&gt;
&lt;!-- html table generated in R 3.4.0 by xtable 1.8-2 package --&gt;
&lt;!-- Wed Jun  7 21:33:23 2017 --&gt;
&lt;table border=&#34;0&#34; width=&#34;100%&#34;&gt;
&lt;tr&gt;
&lt;th&gt;
function
&lt;/th&gt;
&lt;th&gt;
method
&lt;/th&gt;
&lt;th&gt;
data size
&lt;/th&gt;
&lt;th&gt;
time (s)
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
saveRDS
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not compressed
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
500 × 200k
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
5.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
saveRDS
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
compressed
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
500 × 200k
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
51.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
fwrite
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
24 threads
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
500 × 200k
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
41.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
fwrite
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1 thread
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
500 × 200k
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
69.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
fwrite
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
24 threads
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
200k × 500
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
8.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
fwrite
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1 thread
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
200k × 500
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
30.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
readRDS
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
not compressed
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
500 × 200k
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
2.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
readRDS
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
compressed
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
200k × 500
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
11.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
fread
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1 thread
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
500 × 200k
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
55.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;
fread
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
1 thread
&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;
200k × 500
&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;
26.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;For sure, &lt;code&gt;fread&lt;/code&gt; and &lt;code&gt;fwrite&lt;/code&gt; are impressive. And I’d never have thought you could get advantage from parallel reads and writes.&lt;/p&gt;
&lt;p&gt;I’m going to stick with RDS (making use of &lt;code&gt;compress=FALSE&lt;/code&gt; when I don’t care much about disk space) when I want to read/write whole files from R. And I’ll go with SQLite, feather, or fst when I want super fast access to a single row or column. But I also do a lot of reading and writing of CSV files, and I’ve enjoyed &lt;code&gt;data.table::fread&lt;/code&gt; and will now be using &lt;code&gt;data.table::fwrite&lt;/code&gt;, too.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>sqlite, feather, and fst</title>
      <link>http://kbroman.org/blog/2017/04/30/sqlite-feather-and-fst/</link>
      <pubDate>Sun, 30 Apr 2017 14:07:00 -0500</pubDate>
      
      <guid>http://kbroman.org/blog/2017/04/30/sqlite-feather-and-fst/</guid>
      <description>&lt;p&gt;I don’t think I’m unusual among statisticians in having avoided working directly with databases for much of my career. The data for my projects have been reasonably small. (In fact, basically all of the data for my 20 years of projects are on my laptop’s drive.) Flat files (such as CSV files) were sufficient.&lt;/p&gt;
&lt;p&gt;But I’ve finally entered the modern era of biggish data. (Why do they call it &lt;em&gt;big&lt;/em&gt; data? That doesn’t leave us much room for gradations of size. In the 90’s, statisticians talked about &lt;a href=&#34;https://www.nap.edu/read/5505/chapter/1&#34;&gt;&lt;em&gt;massive&lt;/em&gt; data&lt;/a&gt;.) And particularly for visualization of large-scale data, I don’t want to load everything in advance, and I want rapid access to slices of data.&lt;/p&gt;
&lt;p&gt;So I’ve been playing with &lt;a href=&#34;https://www.sqlite.org/&#34;&gt;SQLite&lt;/a&gt; and &lt;a href=&#34;https://www.mongodb.com/&#34;&gt;MongoDB&lt;/a&gt;, and more recently &lt;a href=&#34;https://github.com/wesm/feather&#34;&gt;feather&lt;/a&gt; and &lt;a href=&#34;http://www.fstpackage.org&#34;&gt;fst&lt;/a&gt;. And I thought I’d show a few examples. I’m interested mostly in quick access, from &lt;a href=&#34;https://www.r-project.org&#34;&gt;R&lt;/a&gt;, to small portions of a large file.&lt;/p&gt;
&lt;div id=&#34;data-rds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;data, rds&lt;/h3&gt;
&lt;p&gt;Let me start by simulating some data. I’m mostly thinking about the case of 500 100k-SNP arrays. So the data are pairs of intensity measures for the two alleles at each of 100k SNPs in 500 samples. And typically I want to grab the 500 pairs of intensities for a given SNP. I’m going to just simulate IID noise, because for these illustrations I don’t really care about the contents so much as the storage size and I/O speed.&lt;/p&gt;
&lt;p&gt;(&lt;em&gt;Note: this stuff takes a long time to run, so the &lt;a href=&#34;https://github.com/kbroman/blog/blob/source/content/post/2017-04-30-sqlite-feather-and-fst.Rmd&#34;&gt;actual code behind the scenes&lt;/a&gt; is more complicated, with me having cached the timings and skipped the actual runs.&lt;/em&gt;)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_ind &amp;lt;- 500
n_snps &amp;lt;- 1e5
ind_names &amp;lt;- paste0(&amp;quot;ind&amp;quot;, 1:n_ind)
snp_names &amp;lt;- paste0(&amp;quot;snp&amp;quot;, 1:n_snps)
sigX &amp;lt;- matrix(rnorm(n_ind*n_snps), nrow=n_ind)
sigY &amp;lt;- matrix(rnorm(n_ind*n_snps), nrow=n_ind)
dimnames(sigX) &amp;lt;- list(ind_names, paste0(snp_names, &amp;quot;.X&amp;quot;))
dimnames(sigY) &amp;lt;- list(ind_names, paste0(snp_names, &amp;quot;.Y&amp;quot;))
db &amp;lt;- cbind(data.frame(id=ind_names, stringsAsFactors=FALSE),
            sigX, sigY)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My typical approach with data like this is to save it in an RDS file and just read the whole thing into memory if I want to work with it. But it’s rather slow to write and read such a big data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rds_file &amp;lt;- &amp;quot;db.rds&amp;quot;
saveRDS(db, rds_file)
db_copy &amp;lt;- readRDS(rds_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It was like 49 sec to write the RDS file, and 7.0 sec to read it. The file itself is 769 MB.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sqlite&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;sqlite&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mongodb.com&#34;&gt;Mongo&lt;/a&gt; is cool, and I think ultimately it will be useful to me, but &lt;a href=&#34;https://www.sqlite.org/&#34;&gt;SQLite&lt;/a&gt; has the advantage of being a single file that you can hand to others. And installation is easy: you just need &lt;a href=&#34;https://cran.rstudio.com/package=RSQLite/&#34;&gt;&lt;code&gt;install.packages(&amp;quot;RSQLite&amp;quot;)&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SQLite won’t take more than &lt;a href=&#34;https://sqlite.org/limits.html&#34;&gt;2000 columns&lt;/a&gt; (or maybe 32,767 if you change a compile-time parameter), so we need to take the transpose of our data frame.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;t_db &amp;lt;- cbind(data.frame(snp=rep(snp_names, 2),
                         signal=rep(c(&amp;quot;X&amp;quot;, &amp;quot;Y&amp;quot;), each=n_snps),
                         stringsAsFactors=FALSE),
              rbind(t(sigX), t(sigY)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let’s rearrange it so that the two rows for a given SNP are next to each other.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db_rows &amp;lt;- as.numeric(matrix(1:nrow(t_db), byrow=TRUE, nrow=2))
t_db &amp;lt;- t_db[db_rows,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To write to a SQLite file, we use &lt;code&gt;dbConnect&lt;/code&gt; to create a database connection, and then &lt;code&gt;dbWriteTable&lt;/code&gt;. We can use &lt;code&gt;dbDisconnect&lt;/code&gt; to disconnect afterwards, if we’re done.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(RSQLite)
sqlite_file &amp;lt;- &amp;quot;t_db.sqlite&amp;quot;
sqldb &amp;lt;- dbConnect(SQLite(), dbname=sqlite_file)
dbWriteTable(sqldb, &amp;quot;snps&amp;quot;, t_db, row.names=FALSE, overwrite=TRUE,
             append=FALSE, field.types=NULL)
dbDisconnect(sqldb)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The writing took 35 sec, and the resulting file is 923 MB.&lt;/p&gt;
&lt;p&gt;A key advantage of SQLite is to be able to quickly access a portion of the data, for example to grab the two rows for a particular SNP. You’d need to know the SNP names, first, which you can get by grabbing that column (or &lt;em&gt;field&lt;/em&gt;) with &lt;code&gt;dbGetQuery&lt;/code&gt;. A data frame is returned, so we select the first column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqldb &amp;lt;- dbConnect(SQLite(), dbname=sqlite_file)
snp_names &amp;lt;- dbGetQuery(sqldb, &amp;#39;select snp from snps&amp;#39;)[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can call &lt;code&gt;dbGetQuery&lt;/code&gt; again to get the two rows of data for a given SNP.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_snp &amp;lt;- sample(snp_names, 1)
query &amp;lt;- paste0(&amp;#39;select * from snps where snp == &amp;quot;&amp;#39;, random_snp, &amp;#39;&amp;quot;&amp;#39;)
system.time(z &amp;lt;- dbGetQuery(sqldb, query))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.353   0.485   0.838&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Such queries are faster if we first add an index on the SNP names.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dbGetQuery(sqldb, &amp;quot;CREATE INDEX snp ON snps(snp)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file is basically the same size, 926 MB, and queries are now all but instantaneous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_snp &amp;lt;- sample(snp_names, 1)
query &amp;lt;- paste0(&amp;#39;select * from snps where snp == &amp;quot;&amp;#39;, random_snp, &amp;#39;&amp;quot;&amp;#39;)
system.time(z &amp;lt;- dbGetQuery(sqldb, query))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.004   0.000   0.004&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;feather&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;feather&lt;/h3&gt;
&lt;p&gt;I’d understood &lt;a href=&#34;https://github.com/wesm/feather&#34;&gt;feather&lt;/a&gt; to be a quick way of transferring data between python and R; &lt;a href=&#34;https://simecek.github.io/&#34;&gt;Petr Simacek&lt;/a&gt; convinced me of its more-broad uses, such as to take the place of a single-table database.&lt;/p&gt;
&lt;p&gt;Writing a feather file is surprisingly fast, and reading it back in is even faster.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(feather)
feather_file &amp;lt;- &amp;quot;t_db.feather&amp;quot;
write_feather(t_db, feather_file)
t_db_clone &amp;lt;- read_feather(feather_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That took about 3 sec to write, and 1.0 sec to read, and the file is about 803 MB.&lt;/p&gt;
&lt;p&gt;But queries of particular columns or rows are fast, too. So you can basically use feather like a database.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db_f &amp;lt;- feather(feather_file)
snp_names &amp;lt;- unlist(db_f[,&amp;quot;snp&amp;quot;])
random_snp &amp;lt;- sample(snp_names, 1)
system.time(z &amp;lt;- db_f[snp_names==random_snp,])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.381   0.353   0.735&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this particular case, it’s actually quite a bit faster to work with feather the other way around; that is, in the original format of 500 arrays x 100k SNPs.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;feather_file_2 &amp;lt;- &amp;quot;db.feather&amp;quot;
write_feather(db, feather_file_2)
db_clone &amp;lt;- read_feather(feather_file_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That took about 3 sec to write, and 1.8 sec to read, and the file is about 815 MB.&lt;/p&gt;
&lt;p&gt;Accessing particular rows is just as easy. First a bit of code to grab the SNP names by grabbing the column names, getting rid of the &lt;code&gt;&amp;quot;.X&amp;quot;&lt;/code&gt; or &lt;code&gt;&amp;quot;.Y&amp;quot;&lt;/code&gt; bits at the end, and then taking the first half.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;db_f_2 &amp;lt;- feather(feather_file_2)
snp_names &amp;lt;- sub(&amp;quot;\\.[XY]$&amp;quot;, &amp;quot;&amp;quot;, colnames(db_f_2))
snp_names &amp;lt;- snp_names[1:(length(snp_names)/2)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we grab the data for a random SNP by pasting &lt;code&gt;&amp;quot;.X&amp;quot;&lt;/code&gt; and &lt;code&gt;&amp;quot;.Y&amp;quot;&lt;/code&gt; back onto the SNP name.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_snp &amp;lt;- sample(snp_names, 1)
system.time(z &amp;lt;- db_f_2[,c(&amp;quot;id&amp;quot;, paste0(random_snp, c(&amp;quot;.X&amp;quot;, &amp;quot;.Y&amp;quot;)))])&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.045   0.000   0.045&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that you can also use &lt;a href=&#34;https://github.com/tidyverse/dplyr&#34;&gt;dplyr&lt;/a&gt; with &lt;a href=&#34;https://github.com/wesm/feather&#34;&gt;feather&lt;/a&gt; as if you’ve got an in-memory data frame.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fst&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;fst&lt;/h3&gt;
&lt;p&gt;After &lt;a href=&#34;https://twitter.com/kwbroman/status/855112575373148162&#34;&gt;tweeting about feather&lt;/a&gt;, &lt;a href=&#34;http://dirk.eddelbuettel.com/&#34;&gt;Dirk Eddelbuettel&lt;/a&gt; suggested that I look at the &lt;a href=&#34;http://www.fstpackage.org/&#34;&gt;fst package&lt;/a&gt;. It’s not quite as slick to take data slices, but it’s potentially faster and you can write a compressed file to save disk space.&lt;/p&gt;
&lt;p&gt;Like SQLite, it’s best not to have &lt;em&gt;too&lt;/em&gt; many columns, so we’ll work with the transposed version of the data frame, with SNPs as rows. Writing and reading are fast.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fst)
fst_file &amp;lt;- &amp;quot;db.fst&amp;quot;
write.fst(t_db, fst_file)
t_db_clone &amp;lt;- read.fst(fst_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That took 3.0 sec to write, 1.1 sec to read, and the file is about 803 MB.&lt;/p&gt;
&lt;p&gt;Writing a compressed file is quite a bit slower. Here at 80% compression.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fstcomp_file &amp;lt;- &amp;quot;db_comp.fst&amp;quot;
write.fst(t_db, fstcomp_file, 80)
t_db_clone &amp;lt;- read.fst(fstcomp_file)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That took 6.7 sec to write, 2.7 sec to read, and the file is about 781 MB.&lt;/p&gt;
&lt;p&gt;Doing queries on an &lt;a href=&#34;http://www.fstpackage.org&#34;&gt;fst&lt;/a&gt; file is not quite as slick as for &lt;a href=&#34;https://github.com/wesm/feather&#34;&gt;feather&lt;/a&gt;, but it’s fast. The &lt;code&gt;read.fst&lt;/code&gt; function has a &lt;code&gt;columns&lt;/code&gt; argument to grab particular columns, and &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt; arguments to grab a slice of rows.&lt;/p&gt;
&lt;p&gt;We’ll first grab the &lt;code&gt;snp&lt;/code&gt; column to get the SNP names. And let’s just work with the compressed version of the file. Since &lt;code&gt;read.fst&lt;/code&gt; will return a one-column data frame, we grab the first column to make it a vector.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;snp_names &amp;lt;- read.fst(fstcomp_file, &amp;quot;snp&amp;quot;)[,1]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can choose a random SNP, find the corresponding rows, and then use &lt;code&gt;from&lt;/code&gt; and &lt;code&gt;to&lt;/code&gt; to grab those two rows. You can see it’s useful to have the the pairs of rows for each SNP be contiguous.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_snp &amp;lt;- sample(snp_names, 1)
wh_rows &amp;lt;- which(random_snp == snp_names)
system.time(z &amp;lt;- read.fst(fstcomp_file, from=wh_rows[1], to=wh_rows[2]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    user  system elapsed 
##   0.078   0.011   0.089&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;timings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;timings&lt;/h3&gt;
&lt;p&gt;Let’s use the &lt;a href=&#34;https://cran.r-project.org/package=microbenchmark&#34;&gt;microbenchmark&lt;/a&gt; package to compare timings for grabbing a random SNP. First a bit of set-up.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;random_snp &amp;lt;- sample(snp_names, 1)
library(microbenchmark)
sqlite_file &amp;lt;- &amp;quot;t_db.sqlite&amp;quot;
sqldb &amp;lt;- dbConnect(SQLite(), dbname=sqlite_file)
query &amp;lt;- paste0(&amp;#39;select * from snps where snp == &amp;quot;&amp;#39;, random_snp, &amp;#39;&amp;quot;&amp;#39;)
db_f &amp;lt;- feather(feather_file)
db_f_2 &amp;lt;- feather(feather_file_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the timings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(sqlite=dbGetQuery(sqldb, query),
               feather=db_f[snp_names==random_snp,],
               feather_t=db_f_2[,c(&amp;quot;id&amp;quot;, paste0(random_snp, c(&amp;quot;.X&amp;quot;,&amp;quot;.Y&amp;quot;)))],
               fst={wh_rows &amp;lt;- which(random_snp == snp_names)
                    read.fst(fst_file, from=wh_rows[1], to=wh_rows[2])},
               fstcomp={wh_rows &amp;lt;- which(random_snp == snp_names)
                    read.fst(fstcomp_file, from=wh_rows[1], to=wh_rows[2])},
               times=100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##       expr min  lq mean median  uq  max neval   cld
##     sqlite   3   4    4      4   4    4   100 a    
##    feather 618 641  717    660 766 1215   100     e
##  feather_t  43  47   48     48  49   54   100  b   
##        fst  61  63   80     64  65  341   100   c  
##    fstcomp  91  95  117     95  97  691   100    d&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a summary of all of the results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(results, digits=c(1,1,0,1),
             col.names=c(&amp;quot;write time (s)&amp;quot;, &amp;quot;read time (s)&amp;quot;,
                         &amp;quot;file size (MB)&amp;quot;, &amp;quot;access time (ms)&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;write time (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;read time (s)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;file size (MB)&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;access time (ms)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;rds&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;49.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;769&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;sqlite&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;923&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.7&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;feather&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;803&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;716.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;feather (tr)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;815&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;48.3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;fst&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;803&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;79.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;fst (compr)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;781&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;116.7&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I don’t think we can draw general conclusions about the relative speed and file size of the three approaches from these results. And I think they’re all really useful and interesting.&lt;/p&gt;
&lt;p&gt;But in this particular case, the file compression didn’t really help with &lt;a href=&#34;http://www.fstpackage.org&#34;&gt;fst&lt;/a&gt; and slowed things down. When accessing the data, &lt;a href=&#34;https://github.com/wesm/feather&#34;&gt;feather&lt;/a&gt; was considerably faster than &lt;a href=&#34;http://www.fstpackage.org&#34;&gt;fst&lt;/a&gt; when the data were organized with the SNPs as columns, but was considerably slower when the data were in the opposite orientation. &lt;a href=&#34;https://www.sqlite.org/&#34;&gt;SQLite&lt;/a&gt; has much faster access times, but with a larger file size that takes longer to write.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
