<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.37" />


<title>Fitting linear mixed models for QTL mapping - the stupidest thing...</title>
<meta property="og:title" content="Fitting linear mixed models for QTL mapping - the stupidest thing...">



  







<link rel="stylesheet" href="http://kbroman.org/blog/css/fonts.css" media="all">
<link rel="stylesheet" href="http://kbroman.org/blog/css/main.css" media="all">

<link rel="stylesheet" href="http://kbroman.org/blog/css/custom.css">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="http://kbroman.org/blog/" class="nav-logo">
    <img src="http://kbroman.org/blog/images/karl.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <a href="http://kbroman.org/blog/" class="nav-title">the stupidest thing...</a>

  <ul class="nav-links">
    
    <li><a href="http://kbroman.org/blog/about">
            <img alt="about" src="../../../../images/about_logo.png" class="nav-link"/>
            </a></li>
    
    <li><a href="https://github.com/kbroman">
            <img alt="github" src="../../../../images/github_logo.png" class="nav-link"/>
            </a></li>
    
    <li><a href="https://twitter.com/kwbroman">
            <img alt="twitter" src="../../../../images/twitter_logo.png" class="nav-link"/>
            </a></li>
    
    <li><a href="http://kbroman.org">
            <img alt="web" src="../../../../images/web_logo.png" class="nav-link"/>
            </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">3 min read</span>
    

    <h1 class="article-title">Fitting linear mixed models for QTL mapping</h1>

    
    <span class="article-date">2015/11/24</span>
    

    <div class="article-content">
      <p>Linear mixed models (LMMs) have become widely used for dealing with population structure in human GWAS, and they&rsquo;re becoming increasing important for QTL mapping in model organisms, particularly for the analysis of advanced intercross lines (AIL), which often exhibit variation in the relationships among individuals.</p>

<p>In my efforts on <a href="http://kbroman.org/qtl2">R/qtl2</a>, a reimplementation <a href="http://rqtl.org">R/qtl</a> to better handle high-dimensional data and more complex cross designs, it was clear that I&rsquo;d need to figure out LMMs. But while <a href="http://www.jstatsoft.org/article/view/v067i01">papers explaining the fit of LMMs</a> seem quite explicit and clear, I&rsquo;d never quite turned the corner to actually seeing how I&rsquo;d implement it. In both reading papers and studying code (e.g., <a href="https://github.com/lme4/lme4/">lme4</a>), I&rsquo;d be going along fine and then get completely lost part-way through.</p>

<p>But I now finally understand LMMs, or at least a particular, simple LMM, and I&rsquo;ve been able to write an implementation: the R package <a href="http://kbroman.org/lmmlite">lmmlite</a>.</p>

<p>It seemed worthwhile to write down some of the details.</p>

<!-- more -->

<p>The model I want to fit is <em>y = X b + e</em>, where var(<em>e</em>) = <em>sK + tI</em>, where <em>K</em> is a known kinship matrix and <em>I</em> is the identity matrix. Think of <em>y</em> as a vector of phenotypes and <em>X</em> as a matrix of covariates. Let <em>v = s+t</em> be the residual variance, and let <em>h = s/(s+t) = s/v</em> be the heritability.</p>

<p>First, a shout to <a href="https://github.com/lomereiter">Artem Tarasov</a>, who wrote a <a href="http://lomereiter.github.io/2015/02/16/lmm_cov.html">series of blog posts</a> walking through and explaining the source code for <a href="https://github.com/MicrosoftGenomics/FaST-LMM">FaST-LMM</a> and <a href="https://github.com/nickFurlotte/pylmm">pylmm</a>, and to <a href="http://whatmind.com/">Nick Furlotte</a>, whose <a href="https://github.com/nickFurlotte/pylmm">pylmm</a> code is especially clear and easy-to-read. Only by reading their work did I come to understand these LMMs.</p>

<p>Back to the model fit:</p>

<ul>
<li><p>For a fixed value of the heritability, <em>h</em>, we have var(<em>e</em>) = <em>v[hK + (1-h)I] = vV</em> where <em>V</em> is known. And so we end up with a general least squares problem, which we can fit in order to estimate <em>b</em> and <em>v</em>.</p></li>

<li><p>And actually, if you take the eigen decomposition of <em>K</em>, say <em>K = UDU&rsquo;</em>, it turns out that you can write <em>hK + (1-h)I = hUDU&rsquo; + (1-h)UU&rsquo; = U[hD + (1-h)I]U&rsquo;</em>. That is, the eigenvectors of <em>K</em> are the same as the eigenvectors of <em>hK + (1-h)I</em>. And so if you pre-multiply <em>y</em> and <em>X</em> by <em>U&rsquo;</em>, you end up with a weighted least squares problem, which is way faster to fit than a general least squares problem.</p></li>

<li><p>Having fit the weighted least squares problem to estimate <em>b</em> and <em>v</em>, you can then calculate the corresponding log likelihood (or, better, the restricted log likelihood, if you want to do REML).</p></li>

<li><p>You&rsquo;re then left with a one-dimensional optimization problem (optimizing the log likelihood over <em>h</em>), which you can solve by <a href="https://en.wikipedia.org/wiki/Brent%27s_method">Brent&rsquo;s method</a>.</p></li>

<li><p>That&rsquo;s it!</p></li>
</ul>

<p>It seems quite obvious in retrospect. It&rsquo;s a bit embarrassing that it&rsquo;s taken me so long to come to this understanding.</p>

<p>In <a href="http://kbroman.org/lmmlite">lmmlite</a>, I implemented this algorithm (closely following the code in <a href="https://github.com/nickFurlotte/pylmm">pylmm</a>) twice: in plain R, and then in C++ (using <a href="https://github.com/RcppCore/RcppEigen">RcppEigen</a>, which is an interface to the <a href="http://eigen.tuxfamily.org/index.php?title=Main_Page">Eigen</a> C++ linear algebra library). The plain R code is a bit slower then pylmm; the C++ code is a bit faster. In the C++ code, almost all of the computation time is devoted to the eigen decomposition of the kinship matrix. Once that&rsquo;s done, the rest is super quick.</p>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  
  var disqus_config = function () {
    this.page.url = "http:\/\/kbroman.org\/blog" + location.pathname;
  };
  
  (function() {
   var inIFrame = function() {
      var iframe = true;
      try { iframe = window.self !== window.top; } catch (e) {}
      return iframe;
    };
    if (inIFrame()) return;
    var d = document, s = d.createElement('script');
    s.src = '//kbroman-blog.disqus.com/embed.js'; s.async = true;
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>



</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="https://creativecommons.org/licenses/by/4.0/legalcode"><img src="http://kbroman.org/blog/images/cc-by.svg" height="28" style="vertical-align:middle;"></a>
          </li>
        </ul>
      </footer>

    </div>
    

    

    
  </body>
</html>

