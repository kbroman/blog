---
title: 'reading/writing biggish data, revisited'
author: Karl
date: '2017-05-11T23:50:00-05:00'
categories: ['R']
tags: ['R', 'RDS', 'data.table', 'big data']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(broman)
set.seed(1979300)
```

[Matt Dowle](https://twitter.com/mattdowle?lang=en) encouraged me to
follow up on my [post about sqlite, feather, and
fst](/2017/04/30/sqlite-feather-and-fst/). One thing to emphasize is
that `saveRDS`, by default, uses compression. If you use
`compress=FALSE` you can skip that and it goes _much_ faster. See, for
example, [his post on "Fast csv writing for
R"](https://blog.h2o.ai/2016/04/fast-csv-writing-for-r/). Also see his
[slides from a recent presentation on parallel
fread](https://github.com/Rdatatable/data.table/wiki/talks/BARUG_201704_ParallelFread.pdf).

I'll first generate the same data that I was using before. And note,
as [\@shabbychef](https://twitter.com/shabbychef) [mentioned on
twitter](https://twitter.com/shabbychef/status/858892435820130304), my
iid simulations mean that compression isn't likely to be useful, [as
we saw in my previous
post](/2017/04/30/sqlite-feather-and-fst/).
So don't assume that these results apply generally; compression is
useful much of the time.

```{r simulate_data, eval=FALSE}
n_ind <- 500
n_snps <- 1e5
ind_names <- paste0("ind", 1:n_ind)
snp_names <- paste0("snp", 1:n_snps)
sigX <- matrix(rnorm(n_ind*n_snps), nrow=n_ind)
sigY <- matrix(rnorm(n_ind*n_snps), nrow=n_ind)
dimnames(sigX) <- list(ind_names, paste0(snp_names, ".X"))
dimnames(sigY) <- list(ind_names, paste0(snp_names, ".Y"))
db <- cbind(data.frame(id=ind_names, stringsAsFactors=FALSE),
            sigX, sigY)
```

```{r simulate_data_really, include=FALSE}
cache_file <- "_cache/2017-05-11-reading-writing-etc.RData"
if(file.exists(cache_file)) {
    load(cache_file)
    not_cached <- FALSE
} else {
    <<simulate_data>>
    not_cached <- TRUE
}
```

Now, let's look at the time to write an RDS file, when compressed and
when not. I'm again going to cache my results and just tell you
what happened.

```{r write_rds, eval=FALSE}
rds_file <- "db.rds"
saveRDS(db, rds_file, compress=FALSE)
rds_comp_file <- "db_comp.rds"
saveRDS(db, rds_comp_file)
db_copy1 <- readRDS(rds_file)
db_copy2 <- readRDS(rds_comp_file)
```

```{r write_rds_really, include=FALSE}
if(not_cached) {
    rds_file <- "db.rds"
    write_rds_time <- system.time(saveRDS(db, rds_file, compress=FALSE))
    rds_comp_file <- "db_comp.rds"
    write_rds_comp_time <- system.time(saveRDS(db, rds_comp_file))
    read_rds_time <- system.time(db_copy1 <- readRDS(rds_file))
    read_rds_comp_time <- system.time(db_copy2 <- readRDS(rds_comp_file))
    rds_size <- file.info(rds_file)$size/10^6
    rds_comp_size <- file.info(rds_comp_file)$size/10^6
}
```

Writing the data to an RDS file took
`r myround(write_rds_time[3], 1)` sec when uncompressed and
`r myround(write_rds_comp_time[3], 1)` sec when compressed.
Reading them back in took `r myround(read_rds_time[3], 1)`
sec for the uncompressed file and
`r myround(read_rds_comp_time[3], 1)` sec for the compressed
file. The uncompressed RDS file was `r round(rds_size)` MB, while the
compressed one was `r round(rds_comp_size)` MB.

So, _holy crap_ reading and writing the RDS files is fast when you use
`compress=FALSE`. Don't tell your system administrator I said this,
but if you're working on a server with loads of disk space, for sure
go with `compress=FALSE` with your RDS files. On your laptop where
uncompressed RDS files might get in the way of your music and movie
libraries, you might want to use the compression.

## How about CSV?

[Dirk Eddelbuettel](http://dirk.eddelbuettel.com/) suggested that I
might just use a plain CSV file, since `data.table::fread` and
`data.table::fwrite` are so fast. How fast?

To make use of the multi-threaded version of
[data.table](https://github.com/Rdatatable/data.table/wiki)'s `fread`,
I need version 1.10.5 which is [on
GitHub](https://github.com/rdatatable/data.table). The version on
[CRAN](https://cran.r-project.org)
([1.10.4](https://cran.r-project.org/package=data.table)) has
multi-threaded `fwrite` but only single-threaded `fread`.

But the GitHub version needs to be compiled with OpenMP, and after a
lot of screwing around to do that, I ended up getting segfaults from
`fwrite`, so I just dumped this plan.

So we'll look at multi-threaded `fwrite` but only single-threaded
`fread`. But we can all look forward to the multi-threaded `fread` in
the near future.

For `fwrite`, the number of threads is controlled by the argument
`nThread`. The default is to call `data.table::getDTthreads()` which
detects the maximum number of cores. On my Mac desktop at work, that's
24. I'm going to hard-code it in.

```{r read_write_csv, eval=FALSE}
csv_file <- "db.csv"
library(data.table)
fwrite(db, csv_file, quote=FALSE)
db_copy3 <- data.table::fread(csv_file)
```

```{r read_write_csv_really, include=FALSE}
if(not_cached) {
    csv_file <- "db.csv"
    library(data.table)
    write_csv_time <- system.time(fwrite(db, csv_file, quote=FALSE, nThread=24))
    read_csv_time <- system.time(db_copy3 <- fread(csv_file))
    csv_size <- file.info(csv_file)$size/10^6
}
```

That took `r myround(write_csv_time[3], 1)` sec to write and
`r myround(read_csv_time[3], 1)` sec to read, and the file size is
`r round(csv_size)` MB.

How about if I set `nThread=1` with `fwrite`?

```{r read_write_csv_1thread, eval=FALSE}
fwrite(db, csv_file, quote=FALSE, nThread=1)
```

```{r read_write_csv_1thread_really, include=FALSE}
if(not_cached) {
    write_csv_1thread_time <- system.time(fwrite(db, csv_file, quote=FALSE, nThread=1))
}
```

Single-threaded, `fwrite` took `r myround(write_csv_1thread_time[3], 1)` sec.

But the data set is 500 rows by 200k columns. How about if I used the transpose?

```{r transpose_df, eval=FALSE}
t_db <- cbind(data.frame(snp=rep(snp_names, 2),
                         signal=rep(c("X", "Y"), each=n_snps),
                         stringsAsFactors=FALSE),
              rbind(t(sigX), t(sigY)))
```

```{r transpose_df_really, include=FALSE}
if(not_cached) {
    <<transpose_df>>
}
```

Now to write and read this.

```{r read_write_csv_t, eval=FALSE}
csv_t_file <- "db_t.csv"
fwrite(t_db, csv_t_file, quote=FALSE, nThread=24)
t_db_copy <- fread(csv_t_file)
```

```{r read_write_csv_t_really, include=FALSE}
if(not_cached) {
    csv_t_file <- "db_t.csv"
    write_csv_t_time <- system.time(fwrite(t_db, csv_t_file, quote=FALSE, nThread=24))
    read_csv_t_time <- system.time(t_db_copy <- fread(csv_t_file))
    csv_t_size <- file.info(csv_t_file)$size/10^6
}
```

That took `r myround(write_csv_t_time[3], 1)` sec to write and
`r myround(read_csv_t_time[3], 1)` sec to read, and the file size is
`r round(csv_t_size)` MB.

And how about if I do `fwrite` single-threaded?

```{r read_write_csv_t_1thread, eval=FALSE}
fwrite(t_db, csv_t_file, quote=FALSE, nThread=1)
```

```{r read_write_csv_t_1thread_really, include=FALSE}
if(not_cached) {
    write_csv_t_1thread_time <- system.time(fwrite(t_db, csv_t_file, quote=FALSE, nThread=1))
}
```

Single-threaded, the transposed data took
`r myround(write_csv_t_1thread_time[3], 1)` sec to write.

(I'm not even going to try `read.csv` and `write.csv`.
I'll leave that to the reader.)

Here's a summary of the times:

<style type="text/css">.table { width: 100%; }</style>

```{r, summary_table, echo=FALSE, results="asis"}
tab <- data.frame("function"=c("saveRDS", "saveRDS", "fwrite", "fwrite",
           "fwrite", "fwrite", "readRDS", "readRDS", "fread", "fread"),
           method=c("not compressed", "compressed", "24 threads",
               "1 thread", "24 threads", "1 thread", "not compressed",
               "compressed", "1 thread", "1 thread"),
          data=c("500 \u00d7 200k","500 \u00d7 200k","500 \u00d7 200k","500 \u00d7 200k","200k \u00d7 500","200k \u00d7 500",
               "500 \u00d7 200k","200k \u00d7 500", "500 \u00d7 200k","200k \u00d7 500"),
          time=c(write_rds_time[3], write_rds_comp_time[3],
                 write_csv_time[3], write_csv_1thread_time[3], write_csv_t_time[3],
                 write_csv_t_1thread_time[3], read_rds_time[3], read_rds_comp_time[3],
                 read_csv_time[3], read_csv_t_time[3]))
colnames(tab) <- c("function", "method", "data size", "time (s)")
library(xtable)
print(xtable(tab, digits=1, align=c(rep("center", 4), "right")), type="html",
      html.table.attributes='border=0 width="100%"', include.rownames=FALSE)
```

For sure, `fread` and `fwrite` are impressive. And I'd never have
thought you could get advantage from parallel reads and writes.

I'm going to stick with RDS (making use of `compress=FALSE` when
I don't care much about disk space) when I want to read/write whole
files from R. And I'll go with SQLite, feather, or fst when I want
super fast access to a single row or column. But I also do a lot of reading
and writing of CSV files, and I've enjoyed `data.table::fread`
and will now be using `data.table::fwrite`, too.

```{r save_cache, include=FALSE}
if(not_cached) {
    save(write_rds_time, write_rds_comp_time,
         read_rds_time, read_rds_comp_time,
         rds_size, rds_comp_size,
         write_csv_time, read_csv_time, csv_size,
         write_csv_1thread_time,
         write_csv_t_time, read_csv_t_time, csv_t_size,
         write_csv_t_1thread_time,
         file=cache_file)
}
```

```{r clean_up, include=FALSE}
if(not_cached) {
    unlink(rds_file)
    unlink(rds_comp_file)
    unlink(csv_file)
    unlink(csv_t_file)
}
```
